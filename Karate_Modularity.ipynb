{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# DSC212 â€” Recursive Spectral Modularity Partitioning\n",
        "\n",
        "This notebook implements recursive spectral modularity partitioning (leading eigenvector of the modularity matrix) on Zachary's Karate Club network. It saves snapshot visualizations after each accepted split and records per-node metric evolution (degree centrality, betweenness, closeness, clustering coefficient) across iterations.\n",
        "\n",
        "How to use:\n",
        "- Install required packages if needed (networkx, numpy, scipy, pandas, matplotlib, seaborn).\n",
        "- Run all cells. Outputs (figures + CSVs) will be written to an `outputs/` folder in the notebook directory.\n",
        "- This notebook includes the helpers.py content and also writes it to `src/helpers.py` so you can push the file to your repo directly if you save the notebook outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Uncomment to install requirements in an environment without them\n",
        "# !pip install networkx numpy scipy pandas matplotlib seaborn jupyter jupytext"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%matplotlib inline\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.sparse as sp\n",
        "import scipy.sparse.linalg as spla\n",
        "\n",
        "sns.set(style=\"whitegrid\")\n",
        "OUTPUT_DIR = \"outputs\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Write helpers.py into src/helpers.py so the helper module is included in the notebook outputs\n",
        "os.makedirs('src', exist_ok=True)\n",
        "helpers_code = '''import numpy as np\nimport scipy.sparse as sp\nimport scipy.sparse.linalg as spla\nimport networkx as nx\n\ndef modularity_matrix_for_nodes(G, nodes, total_edges):\n    \"\"\"\n    Build the modularity matrix B restricted to the given node list.\n    B_ij = A_ij - (k_i * k_j) / (2m)\n    degrees k_i and k_j are taken with respect to the whole graph G.\n    total_edges is m (number of edges).\n    Returns dense numpy array of shape (n, n)\n    \"\"\"\n    idx = {n: i for i, n in enumerate(nodes)}\n    n = len(nodes)\n    A = np.zeros((n, n), dtype=float)\n    degrees = np.array([G.degree(n) for n in nodes], dtype=float)\n    # adjacency\n    for u in nodes:\n        for v in G[u]:\n            if v in idx:\n                A[idx[u], idx[v]] = 1.0\n    k_outer = np.outer(degrees, degrees)\n    B = A - k_outer / (2.0 * total_edges)\n    return B\n\n\ndef leading_eigvec(B, which='LA', tol=1e-6):\n    \"\"\"\n    Compute leading eigenvector (largest eigenvalue) of symmetric matrix B.\n    Uses scipy.sparse.linalg.eigsh first, falls back to dense.\n    Returns leading eigenvalue and eigenvector (numpy arrays).\n    \"\"\"\n    n = B.shape[0]\n    if n == 0:\n        return 0.0, np.array([])\n    if n == 1:\n        return B[0,0], np.array([1.0])\n    try:\n        vals, vecs = spla.eigsh(sp.csr_matrix(B), k=1, which=which, tol=tol, maxiter=1000)\n        return float(vals[0]), vecs[:,0]\n    except Exception:\n        vals, vecs = np.linalg.eigh(B)\n        idx = np.argmax(vals)\n        return float(vals[idx]), vecs[:, idx]\n\n\ndef partition_by_leading_eig(G, nodes, m):\n    \"\"\"\n    Given graph G and a list of nodes (community), compute bipartition using leading eigenvector.\n    Returns:\n      - partA (list), partB (list), delta_Q (float), s vector (numpy array of +/-1)\n    \"\"\"\n    B = modularity_matrix_for_nodes(G, nodes, total_edges=m)\n    eigval, eigvec = leading_eigvec(B)\n    if eigvec.size == 0:\n        return [], [], 0.0, np.array([])\n    # Partition by sign of leading eigenvector\n    s = np.where(eigvec >= 0, 1, -1).astype(float)\n    # If all same sign, then no split\n    if np.all(s == 1) or np.all(s == -1):\n        return [], [], 0.0, s\n    # compute modularity change: (1/(4m)) * s^T B s\n    delta_Q = float((s @ B @ s) / (4.0 * m))\n    idx = {i: n for i, n in enumerate(nodes)}\n    partA = [idx[i] for i in range(len(nodes)) if s[i] == 1]\n    partB = [idx[i] for i in range(len(nodes)) if s[i] == -1]\n    return partA, partB, delta_Q, s\n\n\ndef recursive_spectral_partition(G, stop_min_size=1):\n    \"\"\"\n    Recursively partition the graph's nodes using spectral modularity splits.\n    Accept splits only when delta_Q > 0 and both parts are non-empty and size >= stop_min_size.\n\n    Returns:\n      - communities: list of lists (final communities)\n      - snapshots: list of dicts mapping node -> community_id after each accepted split (iteration snapshots)\n    \"\"\"\n    m = G.number_of_edges()\n    # community assignment dict: node -> community_id\n    comm_assign = {n: 0 for n in G.nodes()}\n    # queue of (community_id, node_list)\n    next_comm_id = 1\n    work = [(0, list(G.nodes()))]\n    snapshots = []\n    communities = {0: list(G.nodes())}\n\n    while work:\n        cid, nodes = work.pop(0)\n        if len(nodes) <= stop_min_size:\n            continue\n        partA, partB, delta_Q, s = partition_by_leading_eig(G, nodes, m)\n        if delta_Q > 1e-12 and len(partA) > 0 and len(partB) > 0:\n            # accept split: assign new ids\n            idA = cid  # reuse current id for one side\n            idB = next_comm_id\n            next_comm_id += 1\n            # update communities dict\n            communities[idA] = partA\n            communities[idB] = partB\n            # update assignments\n            for n in partA:\n                comm_assign[n] = idA\n            for n in partB:\n                comm_assign[n] = idB\n            # update work queue for further splitting\n            work.append((idA, partA))\n            work.append((idB, partB))\n            # snapshot after this accepted split (copy)\n            snapshots.append(dict(comm_assign))\n        else:\n            # no split; leave community as is\n            communities[cid] = nodes\n            continue\n\n    # produce final communities list\n    # compress communities dict to unique sets\n    seen = set()\n    final = []\n    for cid, nodes in communities.items():\n        if cid in seen:\n            continue\n        if nodes:\n            final.append(nodes)\n            seen.update(nodes)\n    return final, snapshots\n'''\n\nwith open('src/helpers.py', 'w') as f:\n    f.write(helpers_code)\n\n# also create an __init__.py so `src` is recognized as a package\nwith open('src/__init__.py', 'w') as f:\n    f.write('# package init for src')\n\nprint('Wrote src/helpers.py and src/__init__.py')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def modularity_matrix_for_nodes(G, nodes, total_edges):\n",
        "    \"\"\"\n",
        "    Build modularity matrix B restricted to `nodes`.\n",
        "    B_ij = A_ij - k_i*k_j/(2m) where degrees are computed on the whole graph G.\n",
        "    Returns numpy.ndarray (n x n).\n",
        "    \"\"\"\n",
        "    idx = {n: i for i, n in enumerate(nodes)}\n",
        "    n = len(nodes)\n",
        "    if n == 0:\n",
        "        return np.zeros((0,0))\n",
        "    A = np.zeros((n, n), dtype=float)\n",
        "    degrees = np.array([G.degree(n) for n in nodes], dtype=float)\n",
        "    for u in nodes:\n",
        "        for v in G[u]:\n",
        "            if v in idx:\n",
        "                A[idx[u], idx[v]] = 1.0\n",
        "    k_outer = np.outer(degrees, degrees)\n",
        "    B = A - k_outer / (2.0 * total_edges)\n",
        "    return B\n",
        "\n",
        "def leading_eigvec(B, which='LA', tol=1e-6):\n",
        "    \"\"\"\n",
        "    Compute the leading eigenvalue and eigenvector of symmetric matrix B.\n",
        "    Uses sparse ARPACK via eigsh when possible, falls back to dense eigh.\n",
        "    \"\"\"\n",
        "    n = B.shape[0]\n",
        "    if n == 0:\n",
        "        return 0.0, np.array([])\n",
        "    if n == 1:\n",
        "        return float(B[0,0]), np.array([1.0])\n",
        "    try:\n",
        "        vals, vecs = spla.eigsh(sp.csr_matrix(B), k=1, which=which, tol=tol, maxiter=1000)\n",
        "        return float(vals[0]), vecs[:, 0]\n",
        "    except Exception:\n",
        "        vals, vecs = np.linalg.eigh(B)\n",
        "        idx = np.argmax(vals)\n",
        "        return float(vals[idx]), vecs[:, idx]\n",
        "\n",
        "def partition_by_leading_eig(G, nodes, m):\n",
        "    \"\"\"\n",
        "    Compute bipartition of `nodes` by the sign of leading eigenvector of modularity matrix.\n",
        "    Returns (partA, partB, delta_Q, s)\n",
        "    s is array of +/-1 in the order of `nodes`.\n",
        "    \"\"\"\n",
        "    B = modularity_matrix_for_nodes(G, nodes, total_edges=m)\n",
        "    eigval, eigvec = leading_eigvec(B)\n",
        "    if eigvec.size == 0:\n",
        "        return [], [], 0.0, np.array([])\n",
        "    s = np.where(eigvec >= 0, 1.0, -1.0)\n",
        "    # if all same sign -> no split\n",
        "    if np.all(s == 1.0) or np.all(s == -1.0):\n",
        "        return [], [], 0.0, s\n",
        "    delta_Q = float((s @ B @ s) / (4.0 * m))\n",
        "    idx = {i: n for i, n in enumerate(nodes)}\n",
        "    partA = [idx[i] for i in range(len(nodes)) if s[i] == 1.0]\n",
        "    partB = [idx[i] for i in range(len(nodes)) if s[i] == -1.0]\n",
        "    return partA, partB, delta_Q, s\n",
        "\n",
        "def recursive_spectral_partition(G, stop_min_size=1):\n",
        "    \"\"\"\n",
        "    Recursively partition G using spectral modularity splits.\n",
        "    Accept splits only when delta_Q > 0 and both parts non-empty and meet stop_min_size.\n",
        "    Returns final communities (list of node lists) and snapshots (list of node->comm_id dicts) after each accepted split.\n",
        "    \"\"\"\n",
        "    m = G.number_of_edges()\n",
        "    comm_assign = {n: 0 for n in G.nodes()}\n",
        "    work = [(0, list(G.nodes()))]\n",
        "    next_comm_id = 1\n",
        "    snapshots = []\n",
        "    communities = {0: list(G.nodes())}\n",
        "\n",
        "    while work:\n",
        "        cid, nodes = work.pop(0)\n",
        "        if len(nodes) <= stop_min_size:\n",
        "            continue\n",
        "        partA, partB, delta_Q, s = partition_by_leading_eig(G, nodes, m)\n",
        "        if delta_Q > 1e-12 and len(partA) > 0 and len(partB) > 0:\n",
        "            idA = cid\n",
        "            idB = next_comm_id\n",
        "            next_comm_id += 1\n",
        "            communities[idA] = partA\n",
        "            communities[idB] = partB\n",
        "            for n in partA:\n",
        "                comm_assign[n] = idA\n",
        "            for n in partB:\n",
        "                comm_assign[n] = idB\n",
        "            work.append((idA, partA))\n",
        "            work.append((idB, partB))\n",
        "            snapshots.append(dict(comm_assign))\n",
        "        else:\n",
        "            communities[cid] = nodes\n",
        "\n",
        "    seen_nodes = set()\n",
        "    final = []\n",
        "    for nodes in communities.values():\n",
        "        if not nodes:\n",
        "            continue\n",
        "        nset = tuple(sorted(nodes))\n",
        "        if not set(nset).issubset(seen_nodes):\n",
        "            final.append(list(nset))\n",
        "            seen_nodes.update(nset)\n",
        "    return final, snapshots\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def draw_community_graph(G, comm_assign, title=None, fname=None, seed=42):\n",
        "    plt.figure(figsize=(8,6))\n",
        "    pos = nx.spring_layout(G, seed=seed)\n",
        "    comms = sorted(set(comm_assign.values()))\n",
        "    pal = sns.color_palette(\"tab10\", n_colors=max(3, len(comms)))\n",
        "    color_map = {c: pal[i % len(pal)] for i, c in enumerate(comms)}\n",
        "    node_colors = [color_map[comm_assign[n]] for n in G.nodes()]\n",
        "    nx.draw_networkx_nodes(G, pos, node_size=300, node_color=node_colors)\n",
        "    nx.draw_networkx_edges(G, pos, alpha=0.6)\n",
        "    nx.draw_networkx_labels(G, pos)\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    plt.axis('off')\n",
        "    if fname:\n",
        "        plt.savefig(fname, bbox_inches='tight', dpi=150)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "def compute_node_metrics(G):\n",
        "    deg_c = nx.degree_centrality(G)\n",
        "    betw = nx.betweenness_centrality(G, normalized=True)\n",
        "    close = nx.closeness_centrality(G)\n",
        "    clust = nx.clustering(G)\n",
        "    df = pd.DataFrame({\n",
        "        \"degree_centrality\": pd.Series(deg_c),\n",
        "        \"betweenness_centrality\": pd.Series(betw),\n",
        "        \"closeness_centrality\": pd.Series(close),\n",
        "        \"clustering\": pd.Series(clust),\n",
        "    })\n",
        "    return df\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_analysis(stop_min_size=1, save_plots=True):\n",
        "    G = nx.karate_club_graph()\n",
        "    m = G.number_of_edges()\n",
        "    print(f\"Loaded Karate graph: nodes={G.number_of_nodes()}, edges={m}\")\n",
        "\n",
        "    # initial snapshot (iteration 0)\n",
        "    metrics0 = compute_node_metrics(G)\n",
        "    metrics0.to_csv(os.path.join(OUTPUT_DIR, \"metrics_iteration_0.csv\"))\n",
        "    init_assign = {n: 0 for n in G.nodes()}\n",
        "    if save_plots:\n",
        "        draw_community_graph(G, init_assign, title='Iteration 0 - initial community', fname=os.path.join(OUTPUT_DIR, 'graph_iter_0.png'))\n",
        "\n",
        "    # recursive partitioning\n",
        "    final_comms, snapshots = recursive_spectral_partition(G, stop_min_size=stop_min_size)\n",
        "    # save final communities\n",
        "    with open(os.path.join(OUTPUT_DIR, 'final_communities.txt'), 'w') as f:\n",
        "        for i, c in enumerate(final_comms):\n",
        "            f.write(f\"Community {i}: {sorted(c)}\\n\")\n",
        "\n",
        "    # collect metrics per iteration\n",
        "    all_metrics = []\n",
        "    m0 = metrics0.copy()\n",
        "    m0['iteration'] = 0\n",
        "    m0['community'] = m0.index.map(lambda n: 0)\n",
        "    all_metrics.append(m0.reset_index().rename(columns={'index':'node'}))\n",
        "\n",
        "    for it, snap in enumerate(snapshots, start=1):\n",
        "        if save_plots:\n",
        "            draw_community_graph(G, snap, title=f'Iteration {it}', fname=os.path.join(OUTPUT_DIR, f'graph_iter_{it}.png'))\n",
        "        df = compute_node_metrics(G)\n",
        "        df['iteration'] = it\n",
        "        df['community'] = df.index.map(lambda n: snap.get(n, -1))\n",
        "        all_metrics.append(df.reset_index().rename(columns={'index':'node'}))\n",
        "        df.to_csv(os.path.join(OUTPUT_DIR, f'metrics_iteration_{it}.csv'))\n",
        "\n",
        "    metrics_all = pd.concat(all_metrics, ignore_index=True)\n",
        "    metrics_all.to_csv(os.path.join(OUTPUT_DIR, 'metrics_all_iterations.csv'), index=False)\n",
        "\n",
        "    # Plot evolution lines per node (overlayed)\n",
        "    fig, axes = plt.subplots(2,2, figsize=(12,10))\n",
        "    for metric, ax in zip([\"degree_centrality\",\"betweenness_centrality\",\"closeness_centrality\",\"clustering\"], axes.flatten()):\n",
        "        sns.lineplot(data=metrics_all, x='iteration', y=metric, hue='node', estimator=None, alpha=0.6, legend=False, ax=ax)\n",
        "        ax.set_title(f\"{metric} evolution\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'metrics_evolution_lines.png'), dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    # Boxplots per iteration for each metric\n",
        "    fig, axes = plt.subplots(2,2, figsize=(12,10))\n",
        "    for metric, ax in zip([\"degree_centrality\",\"betweenness_centrality\",\"closeness_centrality\",\"clustering\"], axes.flatten()):\n",
        "        sns.boxplot(data=metrics_all, x='iteration', y=metric, ax=ax)\n",
        "        ax.set_title(f\"{metric} distribution by iteration\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'metrics_evolution_boxplots.png'), dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    print('Saved outputs to', OUTPUT_DIR)\n",
        "    for fname in sorted(os.listdir(OUTPUT_DIR)):\n",
        "        print(' -', fname)\n",
        "\n",
        "    return final_comms, snapshots, metrics_all\n",
        "\n",
        "# Run the analysis\n",
        "final_comms, snapshots, metrics_all = run_analysis(stop_min_size=1, save_plots=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes and suggested next steps\n",
        "\n",
        "- The recursion accepts a split only when it yields positive modularity gain (delta_Q > 0) and both sides are non-empty.\n",
        "- You can adjust `stop_min_size` to require a minimum community size before attempting to split.\n",
        "- The notebook now writes `src/helpers.py` (identical to the helper module used in the script) so you can extract that file directly from the notebook outputs if you need to push a separate helpers.py file to your repository.\n",
        "- If you want, I can also produce a short Markdown report summarizing the discovered communities and notable node metric trends for submission."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
